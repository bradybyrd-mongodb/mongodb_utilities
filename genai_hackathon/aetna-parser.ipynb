{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB - Parsing, Storage, and Vector Search of PDFs\n",
    "\n",
    "1. Import parsing libraries and the MDB driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brady.byrd/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "import nltk\n",
    "nltk.download(\"punkt\")  # Download the necessary data for tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Parse out paragraphs from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    pdf_content = response.content\n",
    "\n",
    "    pdf_file = BytesIO(pdf_content)\n",
    "\n",
    "    text_file = BytesIO()\n",
    "\n",
    "    # Extract the text from the PDF and store it in the file-like object\n",
    "    extract_text_to_fp(pdf_file, text_file, laparams=LAParams())\n",
    "\n",
    "    text_file.seek(0)\n",
    "\n",
    "    extracted_text = text_file.read().decode(\"utf-8\")\n",
    "\n",
    "    # Split the extracted text into separate paragraphs\n",
    "    paragraphs = extracted_text.split(\"\\n\\n\")\n",
    "\n",
    "    # Tokenize sentences from each paragraph\n",
    "    docs = []\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = []\n",
    "        paragraph_sentences = nltk.sent_tokenize(paragraph)\n",
    "        sentences.extend(paragraph_sentences)\n",
    "        docs.append({\"raw\": paragraph, \"sentences\": sentences})\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Parse out paragraphs from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    html_content = response.content\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Find all paragraphs in the HTML document\n",
    "    paragraphs = [p.get_text() for p in soup.find_all(\"p\")]\n",
    "\n",
    "    # Tokenize sentences from each paragraph\n",
    "    docs = []\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = []\n",
    "        paragraph_sentences = nltk.sent_tokenize(paragraph)\n",
    "        sentences.extend(paragraph_sentences)\n",
    "        docs.append({\"raw\": paragraph, \"sentences\": sentences})\n",
    "\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Get documents from the URLs specified in the aetna_plan_docs.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"aetna_plan_docs.csv\"\n",
    "print(\"Opening csv file\")\n",
    "with open(csv_file_path, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)\n",
    "    print(f'Got header: {header}')\n",
    "    client = MongoClient('mongodb+srv://main_admin:<secret>@hackathon.ughh2.mongodb.net')\n",
    "    db = client['vector_search_demo']\n",
    "    collection = db['mpsf_plan_docs']\n",
    "\n",
    "    # Loop through each row in the CSV file\n",
    "    for row in reader:\n",
    "        row_dict = {header[i]: row[i] for i in range(len(header))}\n",
    "        url = row_dict['Content Location']\n",
    "        if url.endswith(\".pdf\"):\n",
    "            parsed_par = parse_pdf(url)\n",
    "            row_dict[\"paragraphs\"] = parsed_par\n",
    "        elif url.endswith((\".html\", \".htm\")):\n",
    "            parsed_par = parse_html(url)\n",
    "            row_dict[\"paragraphs\"] = parsed_par\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported document type\")\n",
    "\n",
    "        collection.insert_one(row_dict)\n",
    "    \n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create the MDB client, loop through the list of URLs and store the data in MDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
